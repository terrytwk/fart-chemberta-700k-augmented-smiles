{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FART Evaluation Pipeline\n",
        "\n",
        "This notebook evaluates a FART (Flavor Aroma Recognition Task) model, trains a RoBERTa classifier, and displays evaluation metrics including ROC curves, confusion matrix, and accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colab setup\n",
        "'''\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# install libraries\n",
        "!pip install -r \"/content/drive/MyDrive/6.7910/code/requirements.txt\"\n",
        "\n",
        "# TODO: make sure to edit data-dir in configuration cell as well\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from rdkit import Chem\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch import nn\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "# Set matplotlib to display plots inline\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8-darkgrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚙️ Configuration\n",
        "\n",
        "**Configure all parameters here before running the notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "model_checkpoint = \"terrytwk/chemberta-700k-augmented-smiles\"\n",
        "tokenizer_checkpoint = \"seyonec/SMILES_tokenized_PubChem_shard00_160k\"\n",
        "data_dir = \"./dataset/splits\"\n",
        "augmentation = False  # Set to True to enable SMILES augmentation\n",
        "augmentation_numbers = [10, 10, 10, 10, 10]  # Only used if augmentation=True\n",
        "num_train_epochs = 2\n",
        "per_device_train_batch_size = 16\n",
        "per_device_eval_batch_size = 16\n",
        "max_length = 512\n",
        "run_name = \"fart_evaluation\"\n",
        "\n",
        "tastes = [\"bitter\", \"sour\", \"sweet\", \"umami\", \"undefined\"]\n",
        "label_column = \"Canonicalized Taste\"\n",
        "smiles_column = \"Canonicalized SMILES\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FART EVALUATION PIPELINE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Model: {model_checkpoint}\")\n",
        "print(f\"Tokenizer: {tokenizer_checkpoint}\")\n",
        "print(f\"Data directory: {data_dir}\")\n",
        "print(f\"Augmentation: {augmentation}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def control_smiles_duplication(random_smiles, duplicate_control=lambda x: 1):\n",
        "    counted_smiles = Counter(random_smiles)\n",
        "    smiles_duplication = {\n",
        "        smiles: math.ceil(duplicate_control(counted_smiles[smiles]))\n",
        "        for smiles in counted_smiles\n",
        "    }\n",
        "    return list(\n",
        "        itertools.chain.from_iterable(\n",
        "            [[smiles] * smiles_duplication[smiles] for smiles in smiles_duplication]\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def smiles_to_random(smiles, int_aug=50):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    if int_aug > 0:\n",
        "        return [\n",
        "            Chem.MolToSmiles(mol, canonical=False, doRandom=True)\n",
        "            for _ in range(int_aug)\n",
        "        ]\n",
        "    if int_aug == 0:\n",
        "        return [smiles]\n",
        "    raise ValueError(\"int_aug must be greater or equal to zero.\")\n",
        "\n",
        "\n",
        "def augmentation_without_duplication(smiles, augmentation_number):\n",
        "    smiles_list = smiles_to_random(smiles, augmentation_number)\n",
        "    return control_smiles_duplication(smiles_list, lambda x: 1)\n",
        "\n",
        "\n",
        "def augment_dataset(dataset: Dataset, augmentation_numbers, tastes, label_column, smiles_column):\n",
        "    augmented_data = []\n",
        "    for i, taste in enumerate(tastes):\n",
        "        for entry in dataset:\n",
        "            if entry[label_column] == taste:\n",
        "                original_smiles = entry[smiles_column]\n",
        "                new_smiles_list = augmentation_without_duplication(original_smiles, augmentation_numbers[i])\n",
        "                for new_smiles in new_smiles_list:\n",
        "                    new_entry = deepcopy(entry)\n",
        "                    new_entry[smiles_column] = new_smiles\n",
        "                    augmented_data.append(new_entry)\n",
        "            else:\n",
        "                augmented_data.append(entry)\n",
        "    return Dataset.from_dict({key: [entry[key] for entry in augmented_data] for key in augmented_data[0]})\n",
        "\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    \"\"\"Trainer with optional class-weighted loss.\"\"\"\n",
        "\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = nn.CrossEntropyLoss(\n",
        "            weight=self.class_weights.to(logits.device) if self.class_weights is not None else None\n",
        "        )\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def load_csvs(data_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    train_df = pd.read_csv(os.path.join(data_dir, \"fart_train.csv\"))\n",
        "    val_df = pd.read_csv(os.path.join(data_dir, \"fart_val.csv\"))\n",
        "    test_df = pd.read_csv(os.path.join(data_dir, \"fart_test.csv\"))\n",
        "    train_df.reset_index(drop=True, inplace=True)\n",
        "    val_df.reset_index(drop=True, inplace=True)\n",
        "    test_df.reset_index(drop=True, inplace=True)\n",
        "    return train_df, val_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[1/7] Loading data...\")\n",
        "train_df, val_df, test_df = load_csvs(data_dir)\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "print(f\"✓ Train samples: {len(train_dataset)}\")\n",
        "print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
        "print(f\"✓ Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if augmentation:\n",
        "    print(\"\\n[2/7] Performing SMILES augmentation...\")\n",
        "    train_dataset = augment_dataset(train_dataset, augmentation_numbers, tastes, label_column, smiles_column)\n",
        "    val_dataset = augment_dataset(val_dataset, augmentation_numbers, tastes, label_column, smiles_column)\n",
        "    test_dataset = augment_dataset(test_dataset, augmentation_numbers, tastes, label_column, smiles_column)\n",
        "    print(f\"✓ Augmented train samples: {len(train_dataset)}\")\n",
        "    print(f\"✓ Augmented validation samples: {len(val_dataset)}\")\n",
        "    print(f\"✓ Augmented test samples: {len(test_dataset)}\")\n",
        "else:\n",
        "    print(\"\\n[2/7] Skipping augmentation...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(f\"\\n[3/7] Loading model and tokenizer from: {model_checkpoint}\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "# print(f\"✓ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\")\n",
        "print(f\"\\n[3/7] Loading model and tokenizer...\")\n",
        "tokenizer_path = tokenizer_checkpoint if tokenizer_checkpoint else model_checkpoint\n",
        "print(f\"  Model checkpoint: {model_checkpoint}\")\n",
        "print(f\"  Tokenizer: {tokenizer_path}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "print(f\"✓ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[4/7] Tokenizing datasets...\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[smiles_column],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "print(\"✓ Tokenization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[5/7] Encoding labels...\")\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_dataset[label_column])\n",
        "val_labels = label_encoder.transform(val_dataset[label_column])\n",
        "test_labels = label_encoder.transform(test_dataset[label_column])\n",
        "\n",
        "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
        "val_dataset = val_dataset.add_column(\"labels\", val_labels)\n",
        "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
        "print(f\"✓ Classes: {label_encoder.classes_}\")\n",
        "\n",
        "# Class weights (disabled to mirror legacy behavior)\n",
        "class_weight_values = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels,\n",
        ")\n",
        "class_weights = None\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "for label, count in zip(unique, counts):\n",
        "    class_name = label_encoder.inverse_transform([label])[0]\n",
        "    print(f\"  {class_name}: {count} samples (weight: {class_weight_values[label]:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[6/7] Setting up training...\")\n",
        "num_labels = len(label_encoder.classes_)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        ")\n",
        "print(f\"✓ Classification head initialized with {num_labels} labels\")\n",
        "\n",
        "# Create a temporary output directory for training checkpoints\n",
        "output_dir = \"./temp_training_output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    run_name=run_name,\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_dir=os.path.join(output_dir, \"logs\"),\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=5,\n",
        "    dataloader_num_workers=8,\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_prefetch_factor=2,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    class_weights=class_weights,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nStarting training...\")\n",
        "trainer.train()\n",
        "print(\"✓ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[7/7] Running evaluation...\")\n",
        "print(\"\\nValidation Results:\")\n",
        "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
        "for key, value in val_results.items():\n",
        "    print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nGenerating test predictions...\")\n",
        "predictions = trainer.predict(test_dataset)\n",
        "probs = softmax(predictions.predictions, axis=1)\n",
        "pred_labels = np.argmax(probs, axis=1)\n",
        "true_labels = predictions.label_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate all metrics\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    true_labels, pred_labels, labels=np.arange(num_labels)\n",
        ")\n",
        "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    true_labels, pred_labels, average=\"macro\", labels=np.arange(num_labels)\n",
        ")\n",
        "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "    true_labels, pred_labels, average=\"weighted\", labels=np.arange(num_labels)\n",
        ")\n",
        "\n",
        "label_names = label_encoder.inverse_transform(range(num_labels))\n",
        "\n",
        "# Display metrics\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nOverall Metrics:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Macro Precision: {precision_macro:.4f}\")\n",
        "print(f\"  Macro Recall: {recall_macro:.4f}\")\n",
        "print(f\"  Macro F1 Score: {f1_macro:.4f}\")\n",
        "print(f\"  Weighted Precision: {precision_weighted:.4f}\")\n",
        "print(f\"  Weighted Recall: {recall_weighted:.4f}\")\n",
        "print(f\"  Weighted F1 Score: {f1_weighted:.4f}\")\n",
        "\n",
        "print(\"\\nPer-Class Metrics:\")\n",
        "for i, (p, r, f, s) in enumerate(zip(precision, recall, f1, support)):\n",
        "    print(f\"  Class {label_names[i]}:\")\n",
        "    print(f\"    Precision: {p:.4f}\")\n",
        "    print(f\"    Recall: {r:.4f}\")\n",
        "    print(f\"    F1 Score: {f:.4f}\")\n",
        "    print(f\"    Support: {s}\")\n",
        "\n",
        "# Create a metrics DataFrame for better visualization\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Class': label_names,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'Support': support\n",
        "})\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Metrics Summary Table\")\n",
        "print(\"=\" * 80)\n",
        "print(metrics_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(\n",
        "    conf_matrix,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=label_names,\n",
        "    yticklabels=label_names,\n",
        ")\n",
        "plt.title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\n",
        "plt.ylabel(\"Actual Label\", fontsize=12)\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ROC Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_labels_bin = label_binarize(true_labels, classes=np.arange(num_labels))\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "auc_scores = {}\n",
        "for i in range(num_labels):\n",
        "    if np.sum(true_labels_bin[:, i]) > 0:\n",
        "        auc = roc_auc_score(true_labels_bin[:, i], probs[:, i])\n",
        "        auc_scores[label_names[i]] = auc\n",
        "        fpr, tpr, _ = roc_curve(true_labels_bin[:, i], probs[:, i])\n",
        "        plt.plot(fpr, tpr, label=f\"{label_names[i]} (AUC = {auc:.4f})\", linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Classifier (AUC = 0.5)\", linewidth=1.5)\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "plt.title(\"ROC Curves for Each Class\", fontsize=16, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAUC Scores:\")\n",
        "for class_name, auc in auc_scores.items():\n",
        "    print(f\"  {class_name}: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble Voting (if augmentation enabled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if augmentation:\n",
        "    print(\"\\nPerforming ensemble voting...\")\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"Standardized SMILES\": test_dataset[\"Standardized SMILES\"],\n",
        "            \"label\": test_dataset[\"labels\"],\n",
        "            \"pred_probs\": list(probs),\n",
        "        }\n",
        "    )\n",
        "    df[\"pred_labels\"] = np.argmax(df[\"pred_probs\"].tolist(), axis=1)\n",
        "\n",
        "    grouped = (\n",
        "        df.groupby(\"Standardized SMILES\")\n",
        "        .agg(\n",
        "            {\n",
        "                \"pred_labels\": lambda x: x.value_counts().idxmax()\n",
        "                if x.value_counts().iloc[0] >= 10\n",
        "                else np.nan,\n",
        "                \"label\": \"first\",\n",
        "            }\n",
        "        )\n",
        "        .dropna()\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    pred_probs_voted = np.array(\n",
        "        [np.mean(df.loc[df[\"Standardized SMILES\"] == smile, \"pred_probs\"], axis=0) for smile in grouped[\"Standardized SMILES\"]]\n",
        "    )\n",
        "    true_labels_voted = grouped[\"label\"].values\n",
        "    pred_labels_voted = grouped[\"pred_labels\"].values\n",
        "\n",
        "    accuracy_voted = accuracy_score(true_labels_voted, pred_labels_voted)\n",
        "    precision_v, recall_v, f1_v, support_v = precision_recall_fscore_support(\n",
        "        true_labels_voted, pred_labels_voted, labels=np.arange(num_labels)\n",
        "    )\n",
        "    precision_macro_v, recall_macro_v, f1_macro_v, _ = precision_recall_fscore_support(\n",
        "        true_labels_voted, pred_labels_voted, average=\"macro\", labels=np.arange(num_labels)\n",
        "    )\n",
        "\n",
        "    print(f\"\\nEnsemble Voting Results:\")\n",
        "    print(f\"  Voted Accuracy: {accuracy_voted:.4f}\")\n",
        "    print(f\"  Voted Macro Precision: {precision_macro_v:.4f}\")\n",
        "    print(f\"  Voted Macro Recall: {recall_macro_v:.4f}\")\n",
        "    print(f\"  Voted Macro F1 Score: {f1_macro_v:.4f}\")\n",
        "\n",
        "    # ROC curves for ensemble voting\n",
        "    true_labels_bin_voted = label_binarize(true_labels_voted, classes=np.arange(num_labels))\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    auc_scores_voted = {}\n",
        "    for i in range(num_labels):\n",
        "        if np.sum(true_labels_bin_voted[:, i]) > 0:\n",
        "            auc = roc_auc_score(true_labels_bin_voted[:, i], pred_probs_voted[:, i])\n",
        "            auc_scores_voted[label_names[i]] = auc\n",
        "            fpr, tpr, _ = roc_curve(true_labels_bin_voted[:, i], pred_probs_voted[:, i])\n",
        "            plt.plot(fpr, tpr, label=f\"{label_names[i]} (AUC = {auc:.4f})\", linewidth=2)\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Classifier (AUC = 0.5)\", linewidth=1.5)\n",
        "    plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "    plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "    plt.title(\"ROC Curves for Each Class (Ensemble Voting)\", fontsize=16, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nEnsemble Voting AUC Scores:\")\n",
        "    for class_name, auc in auc_scores_voted.items():\n",
        "        print(f\"  {class_name}: {auc:.4f}\")\n",
        "else:\n",
        "    print(\"\\nEnsemble voting skipped (augmentation disabled)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
